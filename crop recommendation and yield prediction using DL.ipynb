{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Crop Recommendation"
      ],
      "metadata": {
        "id": "zcdh1BZVPNg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN"
      ],
      "metadata": {
        "id": "OS2bavDtPQ_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/Crop_recommendation.csv')  # Make sure to change the filename to your dataset\n",
        "\n",
        "# Split dataset into features and target variable\n",
        "X = data.drop('label', axis=1)  # Features\n",
        "y = data['label']                # Target variable\n",
        "\n",
        "# Encode categorical target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "_, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(\"Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS-HowOfOPDQ",
        "outputId": "0c4944a4-a0fa-4322-b438-593ddce3af42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9477272629737854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to recommend crop based on user inputs\n",
        "def recommend_crop(temperature, humidity, rainfall, nitrogen, phosphorous, potassium, ph):\n",
        "    user_input = [[temperature, humidity, rainfall, nitrogen, phosphorous, potassium, ph]]\n",
        "    user_input_scaled = scaler.transform(user_input)\n",
        "    predicted_probabilities = model.predict(user_input_scaled)\n",
        "    predicted_class = tf.argmax(predicted_probabilities, axis=1).numpy()\n",
        "    predicted_crop = label_encoder.inverse_transform(predicted_class)\n",
        "    return predicted_crop[0]\n",
        "\n",
        "# Example usage\n",
        "temperature = float(input(\"Enter temperature: \"))\n",
        "humidity = float(input(\"Enter humidity: \"))\n",
        "rainfall = float(input(\"Enter rainfall: \"))\n",
        "nitrogen = float(input(\"Enter nitrogen level: \"))\n",
        "phosphorous = float(input(\"Enter phosphorous level: \"))\n",
        "potassium = float(input(\"Enter potassium level: \"))\n",
        "ph = float(input(\"Enter pH value: \"))\n",
        "\n",
        "recommended_crop = recommend_crop(temperature, humidity, rainfall, nitrogen, phosphorous, potassium, ph)\n",
        "print(\"Recommended crop:\", recommended_crop)\n"
      ],
      "metadata": {
        "id": "IVtYtTiXMDIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN"
      ],
      "metadata": {
        "id": "2Yk6MXkSPzQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/Crop_recommendation.csv')  # Make sure to change the filename to your dataset\n",
        "\n",
        "# Split dataset into features and target variable\n",
        "X = data.drop('label', axis=1)  # Features\n",
        "y = data['label']                # Target variable\n",
        "\n",
        "# Encode categorical target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')  # Softmax for multiclass classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',  # Sparse categorical crossentropy for multiclass classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "_, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(\"Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIJFyMlIPvVV",
        "outputId": "16418bf1-d570-47d5-d583-eabe253053f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9522727131843567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to recommend crop based on user inputs\n",
        "def recommend_crop(temperature, humidity, rainfall, nitrogen, phosphorous, potassium, ph):\n",
        "    user_input = [[temperature, humidity, rainfall, nitrogen, phosphorous, potassium, ph]]\n",
        "    user_input_scaled = scaler.transform(user_input)\n",
        "    predicted_probabilities = model.predict(user_input_scaled)\n",
        "    predicted_class = tf.argmax(predicted_probabilities, axis=1).numpy()\n",
        "    predicted_crop = label_encoder.inverse_transform(predicted_class)\n",
        "    return predicted_crop[0]\n",
        "\n",
        "# Example usage\n",
        "temperature = float(input(\"Enter temperature: \"))\n",
        "humidity = float(input(\"Enter humidity: \"))\n",
        "rainfall = float(input(\"Enter rainfall: \"))\n",
        "nitrogen = float(input(\"Enter nitrogen level: \"))\n",
        "phosphorous = float(input(\"Enter phosphorous level: \"))\n",
        "potassium = float(input(\"Enter potassium level: \"))\n",
        "ph = float(input(\"Enter pH value: \"))\n",
        "\n",
        "recommended_crop = recommend_crop(temperature, humidity, rainfall, nitrogen, phosphorous, potassium, ph)\n",
        "print(\"Recommended crop:\", recommended_crop)\n"
      ],
      "metadata": {
        "id": "L8oxLLPjMUxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN (LSTM)"
      ],
      "metadata": {
        "id": "kFMjzI-GPw13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/Crop_recommendation.csv')  # Make sure to change the filename to your dataset\n",
        "\n",
        "# Split dataset into features and target variable\n",
        "X = data.drop('label', axis=1)  # Features\n",
        "y = data['label']                # Target variable\n",
        "\n",
        "# Encode categorical target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
        "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "_, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
        "print(\"Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R18nCMjSPzIa",
        "outputId": "eebb5cac-3d30-407d-fac4-4d0a6747b67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8931818008422852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# improvised code\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(128, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with more epochs and smaller batch size\n",
        "model.fit(X_train_reshaped, y_train, epochs=20, batch_size=16, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "_, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
        "print(\"Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3jPTMloQkrY",
        "outputId": "54b49654-f696-435b-c96c-cb33cd906d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "110/110 [==============================] - 4s 4ms/step - loss: 2.6645 - accuracy: 0.3653\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 1.0201 - accuracy: 0.6659\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.5155 - accuracy: 0.8051\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.3408 - accuracy: 0.8744\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.2497 - accuracy: 0.9080\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.2223 - accuracy: 0.9210\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.1598 - accuracy: 0.9432\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9432\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.1351 - accuracy: 0.9517\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9563\n",
            "Epoch 11/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9619\n",
            "Epoch 12/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.1012 - accuracy: 0.9648\n",
            "Epoch 13/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0864 - accuracy: 0.9705\n",
            "Epoch 14/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0857 - accuracy: 0.9699\n",
            "Epoch 15/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9750\n",
            "Epoch 16/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.9784\n",
            "Epoch 17/20\n",
            "110/110 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9756\n",
            "Epoch 18/20\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0614 - accuracy: 0.9795\n",
            "Epoch 19/20\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0583 - accuracy: 0.9812\n",
            "Epoch 20/20\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0586 - accuracy: 0.9773\n",
            "Accuracy: 0.9704545736312866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yield Prediction"
      ],
      "metadata": {
        "id": "OSpBzMwtUTvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN"
      ],
      "metadata": {
        "id": "PXJwf8UAQAPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/crop_yield.csv')\n",
        "\n",
        "# Drop 'Production' column\n",
        "df = df.drop('Production', axis=1)\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_features = ['Crop_Year', 'Area', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "category_columns = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=category_columns, drop_first=True)\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop('Yield', axis=1)\n",
        "y = df['Yield']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the DNN model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ImnF-fLQBP_",
        "outputId": "19727ee8-35d2-4259-c1ed-ca9e9270f163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "185/185 [==============================] - 0s 1ms/step\n",
            "Mean Squared Error: 92119.9212140558\n",
            "R-squared: 0.8814494799619909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/crop_yield.csv')\n",
        "\n",
        "# Drop 'Production' column\n",
        "df = df.drop('Production', axis=1)\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_features = ['Crop_Year', 'Area', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "category_columns = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=category_columns, drop_first=True)\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop('Yield', axis=1)\n",
        "y = df['Yield']\n",
        "\n",
        "# Ensure all data is of type float32 for TensorFlow compatibility\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the DNN model with improved architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model with more epochs and batch size\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5try8rxFVEq",
        "outputId": "9baa1fc7-511f-4a48-dcea-b24b1e137e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "185/185 [==============================] - 0s 1ms/step\n",
            "R-squared: 0.9329814970370064\n",
            "Mean Squared Error: 52076.867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN\n",
        "\n"
      ],
      "metadata": {
        "id": "3k4soVjkQIuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/crop_yield.csv')\n",
        "\n",
        "# Drop 'Production' column\n",
        "df = df.drop('Production', axis=1)\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_features = ['Crop_Year', 'Area', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "category_columns = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=category_columns, drop_first=True)\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop('Yield', axis=1)\n",
        "y = df['Yield']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the ANN model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu7WMtlrQLVj",
        "outputId": "280e930c-39e4-4820-99e9-69d569303f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "185/185 [==============================] - 0s 1ms/step\n",
            "Mean Squared Error: 99440.09433091257\n",
            "R-squared: 0.8720290384512439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# improvised code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/crop_yield.csv')\n",
        "\n",
        "# Drop 'Production' column\n",
        "df = df.drop('Production', axis=1)\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_features = ['Crop_Year', 'Area', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "category_columns = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=category_columns, drop_first=True)\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop('Yield', axis=1)\n",
        "y = df['Yield']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the ANN model with improved architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model with more epochs and batch size\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPyg5_qEU6_c",
        "outputId": "e91d5960-d533-4e69-ee18-bb0bb90071d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "185/185 [==============================] - 0s 1ms/step\n",
            "R-squared: 0.941170572965534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/crop_yield.csv')\n",
        "\n",
        "# Drop 'Production' column\n",
        "df = df.drop('Production', axis=1)\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_features = ['Crop_Year', 'Area', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "category_columns = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=category_columns, drop_first=True)\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop('Yield', axis=1)\n",
        "y = df['Yield']\n",
        "\n",
        "# Ensure all data is of type float32 for TensorFlow compatibility\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the ANN model with improved architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model with more epochs and batch size\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE57eS5VGQmT",
        "outputId": "617cd7bd-1b8e-4eec-fd0c-61fc37130a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "185/185 [==============================] - 0s 1ms/step\n",
            "R-squared: 0.9424069082561637\n",
            "Mean Squared Error: 44752.824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN (LSTM)"
      ],
      "metadata": {
        "id": "ND9G8YqtSltt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/crop_yield.csv')\n",
        "\n",
        "# Drop 'Production' column\n",
        "df = df.drop('Production', axis=1)\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_features = ['Crop_Year', 'Area', 'Annual_Rainfall', 'Fertilizer', 'Pesticide']\n",
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "category_columns = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=category_columns, drop_first=True)\n",
        "\n",
        "# Split features and target variable\n",
        "X = df.drop('Yield', axis=1)\n",
        "y = df['Yield']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X_train_reshaped = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "# Model Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tKryeilSokq",
        "outputId": "dd173d01-8f67-4fbd-caa9-9606d4bdbbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "185/185 [==============================] - 0s 2ms/step\n",
            "Mean Squared Error: 79490.38220496017\n",
            "R-squared: 0.8977026247501798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# improvised code\n",
        "# Define the model with increased complexity and regularization\n",
        "model = Sequential([\n",
        "    LSTM(128, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True),\n",
        "    LSTM(64, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Train the model with more epochs and smaller batch size\n",
        "model.fit(X_train_reshaped, y_train, epochs=20, batch_size=16, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "# Model Evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P0beYfhSszb",
        "outputId": "4f7eef90-24fa-40c2-8a12-e2a341b4a36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "862/862 [==============================] - 11s 9ms/step - loss: 541682.0625\n",
            "Epoch 2/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 71234.2422\n",
            "Epoch 3/20\n",
            "862/862 [==============================] - 5s 6ms/step - loss: 50566.5508\n",
            "Epoch 4/20\n",
            "862/862 [==============================] - 5s 5ms/step - loss: 41261.5820\n",
            "Epoch 5/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 32130.8320\n",
            "Epoch 6/20\n",
            "862/862 [==============================] - 5s 6ms/step - loss: 43692.1016\n",
            "Epoch 7/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 37156.6445\n",
            "Epoch 8/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 29553.6504\n",
            "Epoch 9/20\n",
            "862/862 [==============================] - 5s 6ms/step - loss: 38158.8047\n",
            "Epoch 10/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 35139.6289\n",
            "Epoch 11/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 47195.0820\n",
            "Epoch 12/20\n",
            "862/862 [==============================] - 5s 6ms/step - loss: 35330.7461\n",
            "Epoch 13/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 34681.0352\n",
            "Epoch 14/20\n",
            "862/862 [==============================] - 5s 6ms/step - loss: 33488.5703\n",
            "Epoch 15/20\n",
            "862/862 [==============================] - 5s 6ms/step - loss: 33343.8555\n",
            "Epoch 16/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 35274.2188\n",
            "Epoch 17/20\n",
            "862/862 [==============================] - 5s 6ms/step - loss: 29850.2910\n",
            "Epoch 18/20\n",
            "862/862 [==============================] - 4s 5ms/step - loss: 39657.4688\n",
            "Epoch 19/20\n",
            "862/862 [==============================] - 4s 4ms/step - loss: 29715.5430\n",
            "Epoch 20/20\n",
            "862/862 [==============================] - 5s 6ms/step - loss: 34311.4219\n",
            "185/185 [==============================] - 1s 2ms/step\n",
            "Mean Squared Error: 56200.70647924444\n",
            "R-squared: 0.9276744607267278\n"
          ]
        }
      ]
    }
  ]
}